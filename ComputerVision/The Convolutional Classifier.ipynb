{"metadata":{"jupytext":{"formats":"ipynb,md","split_at_heading":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1338830,"sourceType":"datasetVersion","datasetId":708136},{"sourceId":1363948,"sourceType":"datasetVersion","datasetId":701538},{"sourceId":1495782,"sourceType":"datasetVersion","datasetId":878523}],"dockerImageVersionId":30648,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**This notebook is an exercise in the [Computer Vision](https://www.kaggle.com/learn/computer-vision) course.  You can reference the tutorial at [this link](https://www.kaggle.com/ryanholbrook/the-convolutional-classifier).**\n\n---\n","metadata":{}},{"cell_type":"markdown","source":"<blockquote style=\"margin-right:auto; margin-left:auto; padding: 1em; margin:24px;\">\n    <strong>Accelerate Training with a Kaggle GPU!</strong><br>\nDid you know Kaggle offers free time with a GPU accelerator? You can speed up training neural networks in this course by switching to <strong>GPU</strong> in the <em>Accelerator</em> option on the right. (It may already be turned on.) Two things to be aware of:\n<ul>\n<li>Changing the <em>Accelerator</em> option will cause the notebook session to restart. You'll need to rerun any setup code.\n<li>You can have only one GPU session at a time, so be sure to shut the notebook down after you've finished the exercise.\n</ul>\n</blockquote>","metadata":{}},{"cell_type":"markdown","source":"# Introduction #\n\nIn the tutorial, we saw how to build an image classifier by attaching a head of dense layers to a pretrained base. The base we used was from a model called **VGG16**. We saw that the VGG16 architecture was prone to overfitting this dataset. Over this course, you'll learn a number of ways you can improve upon this initial attempt.\n\nThe first way you'll see is to use a base more appropriate to the dataset. The base this model comes from is called **InceptionV1** (also known as GoogLeNet). InceptionV1 was one of the early winners of the ImageNet competition. One of its successors, InceptionV4, is among the state of the art today.\n\nTo get started, run the code cell below to set everything up.","metadata":{}},{"cell_type":"code","source":"# Setup feedback system\nfrom learntools.core import binder\nbinder.bind(globals())\nfrom learntools.computer_vision.ex1 import *\n\n# Imports\nimport os, warnings\nimport matplotlib.pyplot as plt\nfrom matplotlib import gridspec\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing import image_dataset_from_directory\n\n# Reproducability\ndef set_seed(seed=31415):\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\nset_seed()\n\n# Set Matplotlib defaults\nplt.rc('figure', autolayout=True)\nplt.rc('axes', labelweight='bold', labelsize='large',\n       titleweight='bold', titlesize=18, titlepad=10)\nplt.rc('image', cmap='magma')\nwarnings.filterwarnings(\"ignore\") # to clean up output cells\n\n\n# Load training and validation sets\nds_train_ = image_dataset_from_directory(\n    '../input/car-or-truck/train',\n    labels='inferred',\n    label_mode='binary',\n    image_size=[128, 128],\n    interpolation='nearest',\n    batch_size=64,\n    shuffle=True,\n)\nds_valid_ = image_dataset_from_directory(\n    '../input/car-or-truck/valid',\n    labels='inferred',\n    label_mode='binary',\n    image_size=[128, 128],\n    interpolation='nearest',\n    batch_size=64,\n    shuffle=False,\n)\n\n# Data Pipeline\ndef convert_to_float(image, label):\n    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    return image, label\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE\nds_train = (\n    ds_train_\n    .map(convert_to_float)\n    .cache()\n    .prefetch(buffer_size=AUTOTUNE)\n)\nds_valid = (\n    ds_valid_\n    .map(convert_to_float)\n    .cache()\n    .prefetch(buffer_size=AUTOTUNE)\n)\n","metadata":{"lines_to_next_cell":2,"execution":{"iopub.status.busy":"2024-10-07T04:45:38.084141Z","iopub.execute_input":"2024-10-07T04:45:38.084513Z","iopub.status.idle":"2024-10-07T04:46:04.226139Z","shell.execute_reply.started":"2024-10-07T04:45:38.084483Z","shell.execute_reply":"2024-10-07T04:46:04.224897Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-10-07 04:45:41.737065: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-10-07 04:45:41.737210: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-10-07 04:45:41.911886: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Found 5117 files belonging to 2 classes.\nFound 5051 files belonging to 2 classes.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The **InceptionV1** model pretrained on ImageNet is available in the [TensorFlow Hub](https://www.tensorflow.org/hub/) repository, but we'll load it from a local copy. Run this cell to load InceptionV1 for your base.","metadata":{}},{"cell_type":"code","source":"import tensorflow_hub as hub\n\npretrained_base = tf.keras.models.load_model(\n    '../input/cv-course-models/cv-course-models/inceptionv1'\n)","metadata":{"lines_to_next_cell":0,"execution":{"iopub.status.busy":"2024-10-07T04:46:04.228388Z","iopub.execute_input":"2024-10-07T04:46:04.228748Z","iopub.status.idle":"2024-10-07T04:46:08.922232Z","shell.execute_reply.started":"2024-10-07T04:46:04.228711Z","shell.execute_reply":"2024-10-07T04:46:08.921214Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# 1) Define Pretrained Base #\n\nNow that you have a pretrained base to do our feature extraction, decide whether this base should be trainable or not.","metadata":{}},{"cell_type":"code","source":"# YOUR_CODE_HERE\npretrained_base.trainable = False\n\n# Check your answer\nq_1.check()","metadata":{"lines_to_next_cell":0,"execution":{"iopub.status.busy":"2024-10-07T04:47:21.545162Z","iopub.execute_input":"2024-10-07T04:47:21.545560Z","iopub.status.idle":"2024-10-07T04:47:21.555695Z","shell.execute_reply.started":"2024-10-07T04:47:21.545528Z","shell.execute_reply":"2024-10-07T04:47:21.554531Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"outcomeType\": 1, \"valueTowardsCompletion\": 0.25, \"interactionType\": 1, \"questionType\": 2, \"questionId\": \"1_Q1\", \"learnToolsVersion\": \"0.3.4\", \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\"}}, \"*\")"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Correct: When doing transfer learning, it's generally not a good idea to retrain the entire base -- at least not without some care. The reason is that the random weights in the head will initially create large gradient updates, which propogate back into the base layers and destroy much of the pretraining. Using techniques known as **fine tuning** it's possible to further train the base on new data, but this requires some care to do well.","text/markdown":"<span style=\"color:#33cc33\">Correct:</span> When doing transfer learning, it's generally not a good idea to retrain the entire base -- at least not without some care. The reason is that the random weights in the head will initially create large gradient updates, which propogate back into the base layers and destroy much of the pretraining. Using techniques known as **fine tuning** it's possible to further train the base on new data, but this requires some care to do well."},"metadata":{}}]},{"cell_type":"code","source":"# Lines below will give you a hint or solution code\n#q_1.hint()\n#q_1.solution()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2) Attach Head #\n\nNow that the base is defined to do the feature extraction, create a head of `Dense` layers to perform the classification, following this diagram:\n\n<figure>\n<img src=\"https://storage.googleapis.com/kaggle-media/learn/images/i5VU7Ry.png\" alt=\"Diagram of the dense head.\">\n</figure>\n","metadata":{}},{"cell_type":"code","source":"from tensorflow import keras\nfrom tensorflow.keras import layers\n\nmodel = keras.Sequential([\n    pretrained_base,\n    layers.Flatten(),\n    layers.Dense(6, activation='relu'),\n    layers.Dense(1, activation='sigmoid'),\n])\n\n# Check your answer\nq_2.check()","metadata":{"lines_to_next_cell":0,"execution":{"iopub.status.busy":"2024-10-07T04:47:42.649504Z","iopub.execute_input":"2024-10-07T04:47:42.650417Z","iopub.status.idle":"2024-10-07T04:47:42.670842Z","shell.execute_reply.started":"2024-10-07T04:47:42.650379Z","shell.execute_reply":"2024-10-07T04:47:42.669682Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"outcomeType\": 1, \"valueTowardsCompletion\": 0.25, \"interactionType\": 1, \"questionType\": 2, \"questionId\": \"2_Q2\", \"learnToolsVersion\": \"0.3.4\", \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\"}}, \"*\")"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Correct","text/markdown":"<span style=\"color:#33cc33\">Correct</span>"},"metadata":{}}]},{"cell_type":"code","source":"# Lines below will give you a hint or solution code\n#q_2.hint()\n#q_2.solution()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3) Train #\n\nBefore training a model in Keras, you need to specify an *optimizer* to perform the gradient descent, a *loss function* to be minimized, and (optionally) any *performance metrics*. The optimization algorithm we'll use for this course is called [\"Adam\"](https://keras.io/api/optimizers/adam/), which generally performs well regardless of what kind of problem you're trying to solve.\n\nThe loss and the metrics, however, need to match the kind of problem you're trying to solve. Our problem is a **binary classification** problem: `Car` coded as 0, and `Truck` coded as 1. Choose an appropriate loss and an appropriate accuracy metric for binary classification.","metadata":{}},{"cell_type":"code","source":"# YOUR CODE HERE: what loss function should you use for a binary\n# classification problem? (Your answer for each should be a string.)\nimport tensorflow as tf\noptimizer = tf.keras.optimizers.Adam(epsilon=0.01)\n\n\n# You can now compile your model using these definitions\nmodel.compile(optimizer=optimizer,\n              loss='binary_crossentropy',\n              metrics=['binary_accuracy'])  # Optionally, include any other metrics you want to track\n\n# Check your answer\nq_3.check()","metadata":{"lines_to_next_cell":0,"execution":{"iopub.status.busy":"2024-10-07T04:52:07.458028Z","iopub.execute_input":"2024-10-07T04:52:07.458915Z","iopub.status.idle":"2024-10-07T04:52:07.482018Z","shell.execute_reply.started":"2024-10-07T04:52:07.458866Z","shell.execute_reply":"2024-10-07T04:52:07.480773Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"outcomeType\": 1, \"valueTowardsCompletion\": 0.25, \"interactionType\": 1, \"questionType\": 2, \"questionId\": \"3_Q3\", \"learnToolsVersion\": \"0.3.4\", \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\"}}, \"*\")"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Correct","text/markdown":"<span style=\"color:#33cc33\">Correct</span>"},"metadata":{}}]},{"cell_type":"code","source":"# Lines below will give you a hint or solution code\n#q_3.hint()\n#q_3.solution()","metadata":{"lines_to_next_cell":0},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(\n    ds_train,\n    validation_data=ds_valid,\n    epochs=10,\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-07T04:55:37.563323Z","iopub.execute_input":"2024-10-07T04:55:37.564161Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/10\n 9/80 [==>...........................] - ETA: 42s - loss: 0.4487 - binary_accuracy: 0.8472","output_type":"stream"}]},{"cell_type":"markdown","source":"Run the cell below to plot the loss and metric curves for this training run.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nhistory_frame = pd.DataFrame(history.history)\nhistory_frame.loc[:, ['loss', 'val_loss']].plot()\nhistory_frame.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot();","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4) Examine Loss and Accuracy #\n\nDo you notice a difference between these learning curves and the curves for VGG16 from the tutorial? What does this difference tell you about what this model (InceptionV2) learned compared to VGG16? Are there ways in which one is better than the other? Worse?\n\nAfter you've thought about it, run the cell below to see the answer.","metadata":{}},{"cell_type":"code","source":"# View the solution (Run this code cell to receive credit!)\nq_4.check()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion #\n\nIn this first lesson, you learned the basics of **convolutional image classifiers**, that they consist of a **base** for extracting features from images, and a **head** which uses the features to decide the image's class. You also saw how to build a classifier with **transfer learning** on pretrained base. ","metadata":{}},{"cell_type":"markdown","source":"# Keep Going #\n\nMove on to [**Lesson 2**](https://www.kaggle.com/ryanholbrook/convolution-and-relu) for a detailed look at how the base does this feature extraction. (It's really cool!)","metadata":{}},{"cell_type":"markdown","source":"---\n\n\n\n\n*Have questions or comments? Visit the [course discussion forum](https://www.kaggle.com/learn/computer-vision/discussion) to chat with other learners.*","metadata":{}}]}